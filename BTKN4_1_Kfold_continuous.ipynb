{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPGufkxcZF8cjZ4/0aHI751",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vohai2003/Pythoncode/blob/main/BTKN4_1_Kfold_continuous.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "T9Wa76LEOokV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae7772ff-e68d-4afb-c346-f7387232005b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /mnt\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/mnt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn.metrics\n",
        "from sklearn.linear_model import LinearRegression,Lasso\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "q6V1ERoWO4V0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "D = numpy.genfromtxt(\"/mnt/My Drive/Dataset/ex1data2.txt\",delimiter=\",\")\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(D)\n",
        "X,y = D[:,:-1], D[:,-1]\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=30)"
      ],
      "metadata": {
        "id": "hLoo7AyD0K0r"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kf = KFold(n_splits=10)\n",
        "model = LinearRegression()\n",
        "step = 0"
      ],
      "metadata": {
        "id": "gqgfAsWj0P0s"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for train_index, val_index in kf.split(X=X_train,y=y_train):\n",
        "  step += 1\n",
        "  X_singtrain, X_singval = X_train[train_index], X_train[val_index]\n",
        "  y_singtrain, y_singval = y_train[train_index], y_train[val_index]\n",
        "  model.fit(X_singtrain,y_singtrain)\n",
        "  yhat = model.predict(X_singval)\n",
        "  yhat_test = model.predict(X_test)\n",
        "  val_loss = mean_squared_error(y_singval,yhat)\n",
        "  test_loss = mean_squared_error(y_test,yhat_test)\n",
        "  print(\"Step: {:<10} val_loss: {:<10} test_loss: {:<10}\".format(step,val_loss,test_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRY5MYlp1P7n",
        "outputId": "471e2d5c-a098-43c6-dfc7-5cd22aee904e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 1          val_loss: 5202734118.655806 test_loss: 4537002581.073696\n",
            "Step: 2          val_loss: 4229536282.3030725 test_loss: 4816230593.106419\n",
            "Step: 3          val_loss: 2088826363.3395078 test_loss: 4759561320.249105\n",
            "Step: 4          val_loss: 9465348099.063633 test_loss: 5081299083.038312\n",
            "Step: 5          val_loss: 21216562279.370193 test_loss: 8080089741.594469\n",
            "Step: 6          val_loss: 4194729894.4893346 test_loss: 4655711144.2112255\n",
            "Step: 7          val_loss: 2147458614.0355954 test_loss: 4550173760.788771\n",
            "Step: 8          val_loss: 17527112065.8073 test_loss: 4545374322.726393\n",
            "Step: 9          val_loss: 23122665.267594624 test_loss: 4680221691.5304365\n",
            "Step: 10         val_loss: 2208874234.9507813 test_loss: 4699095342.43166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hàm thực hiện K-fold CV"
      ],
      "metadata": {
        "id": "Y_11BmUn-8b7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Kfold_continuous(model,k:int,X,y):\n",
        "  kf = KFold(n_splits=k)\n",
        "  step = 0\n",
        "  X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=30,random_state=140)\n",
        "  for train_index, val_index in kf.split(X=X_train,y=y_train):\n",
        "    step += 1\n",
        "    X_singtrain, X_singval = X_train[train_index], X_train[val_index]\n",
        "    y_singtrain, y_singval = y_train[train_index], y_train[val_index]\n",
        "    model.fit(X_singtrain,y_singtrain)\n",
        "    yhat = model.predict(X_singval)\n",
        "    val_loss = mean_squared_error(y_singval,yhat)\n",
        "  yhat_test = model.predict(X_test)\n",
        "  test_loss = mean_squared_error(y_test,yhat_test)\n",
        "  return test_loss"
      ],
      "metadata": {
        "id": "Og9a6MJU-lMh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thực hiện K-fold CV với mô hình Lasso để điều chỉnh hệ số $\\alpha$ của mô hình"
      ],
      "metadata": {
        "id": "WVrFWsa5_YPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "patient = 30\n",
        "working = True\n",
        "alpha = 0\n",
        "memory = numpy.array([1e+99]*patient)\n",
        "min = 1e+99\n",
        "min_alpha = -1\n",
        "while working:\n",
        "  lasso_model = Lasso(alpha = alpha)\n",
        "  eval = Kfold_continuous(lasso_model,10,X,y)\n",
        "  if eval < min:\n",
        "    min = eval\n",
        "    min_alpha = alpha\n",
        "  memory = numpy.append(memory[1:],eval)\n",
        "  # đoạn lệnh dưới đây dùng để dừng vòng lặp khi gặp giá trị alpha phù hợp\n",
        "  if numpy.sum(memory == min) == 0: #tìm giá trị của min trong dãy dữ liệu nhớ\n",
        "    working = False\n",
        "  print(\"alpha:\",alpha,\"eval_score:\",eval)\n",
        "  alpha += 1\n",
        "print(\"Suitable alpha:\",min_alpha)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKN5899J_pYi",
        "outputId": "915c8c2d-dca1-4b3f-ae76-9c6456680efc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.814e+10, tolerance: 2.190e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.211e+10, tolerance: 3.347e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.183e+10, tolerance: 2.784e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.569e+10, tolerance: 3.248e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.759e+10, tolerance: 3.042e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.953e+10, tolerance: 3.122e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.927e+10, tolerance: 3.002e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.248e+10, tolerance: 3.366e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.232e+10, tolerance: 2.943e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.968e+10, tolerance: 3.076e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alpha: 0 eval_score: 5011736142.941385\n",
            "alpha: 1 eval_score: 3616419100.95264\n",
            "alpha: 2 eval_score: 5307402612.830865\n",
            "alpha: 3 eval_score: 4518924107.648428\n",
            "alpha: 4 eval_score: 4031193184.728183\n",
            "alpha: 5 eval_score: 5007363540.648159\n",
            "alpha: 6 eval_score: 4986609460.909272\n",
            "alpha: 7 eval_score: 4729881387.005136\n",
            "alpha: 8 eval_score: 5034457764.1383705\n",
            "alpha: 9 eval_score: 9975931910.985558\n",
            "alpha: 10 eval_score: 5181134900.486481\n",
            "alpha: 11 eval_score: 5900224503.523784\n",
            "alpha: 12 eval_score: 4799548743.7085705\n",
            "alpha: 13 eval_score: 7628060798.993717\n",
            "alpha: 14 eval_score: 4618861721.5142765\n",
            "alpha: 15 eval_score: 4785902022.372976\n",
            "alpha: 16 eval_score: 5143799557.596684\n",
            "alpha: 17 eval_score: 6254212643.96902\n",
            "alpha: 18 eval_score: 7044902489.26907\n",
            "alpha: 19 eval_score: 5036198989.977551\n",
            "alpha: 20 eval_score: 5368137440.590323\n",
            "alpha: 21 eval_score: 5936333455.014178\n",
            "alpha: 22 eval_score: 6018565992.649867\n",
            "alpha: 23 eval_score: 4723895046.752321\n",
            "alpha: 24 eval_score: 6506887091.350585\n",
            "alpha: 25 eval_score: 4543507055.712417\n",
            "alpha: 26 eval_score: 4068394810.2843\n",
            "alpha: 27 eval_score: 5984153470.449649\n",
            "alpha: 28 eval_score: 4907588201.523164\n",
            "alpha: 29 eval_score: 6303153800.213886\n",
            "alpha: 30 eval_score: 6382272462.184271\n",
            "alpha: 31 eval_score: 7541462312.047495\n",
            "Suitable alpha: 1\n"
          ]
        }
      ]
    }
  ]
}